<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,minimum-scale=1,maximum-scale=1"><link href=/css/fonts.css rel=stylesheet type=text/css><title>So You Wanna Use Your Own LLMs in GitHub Copilot Chat</title><link rel=stylesheet href=/css/hugo-octopress.css><link rel=stylesheet href=/css/fork-awesome.min.css><link href=https://parsiya.net/favicon.png rel=icon><meta name=description content><meta name=keywords content="[Parsia Hakimian Parsiya infosec information security]"><meta name=author content="Parsia"><meta name=generator content="Hugo 0.152.2"><meta name=twitter:card content="summary"><meta name=twitter:title content="So You Wanna Use Your Own LLMs in GitHub Copilot Chat"><meta name=twitter:description content="We want to use custom OpenAI compatible API LLMs with GitHub Copilot Chat in VS
Code without API keys. We will use LiteLLM as a proxy for authentication and use
the Azure AI model support in Chat as a hack.

Problem Statement

GitHub Copilot Chat in VS Code (moving forward, called Chat1) allows custom
LLM deployments, but only supports API keys and not AAD/Entra ID. API keys are
icky and not cool anymore. Using this method I can:"><meta name=twitter:domain content="parsiya.net"><meta name=twitter:creator content="@CryptoGangsta"></head><body><header role=banner><hgroup><h1><a href=https://parsiya.net/>Hackerman's Hacking Tutorials</a></h1><h2>The knowledge of anything, since all things have causes, is not acquired or
complete unless it is known by its causes. - Avicenna</h2></hgroup></header><nav role=navigation><fieldset class=mobile-nav><select onchange="location=this.value"><option value>Navigate…</option><option value=https://parsiya.net/about/>» About Me!</option><option value=https://parsiya.net/cheatsheet/>» Cheat Sheet</option><option value=https://parsiya.io/>» My Clone</option><option value=https://github.com/parsiya/parsiya.net>» Source Repo</option><option value="https://queue.acm.org/detail.cfm?id=3197520">» Manual Work is a Bug</option><option value="https://www.google.com/search?q=andrew+ridgeley">» The Other Guy from Wham!</option></select></fieldset><ul class=main-navigation><li><a href=https://parsiya.net/about/ title="About Me!" target=_blank rel="noopener noreferrer">About Me!</a></li><li><a href=https://parsiya.net/cheatsheet/ title="Cheat Sheet" target=_blank rel="noopener noreferrer">Cheat Sheet</a></li><li><a href=https://parsiya.io/ title="My Clone" target=_blank rel="noopener noreferrer">My Clone</a></li><li><a href=https://github.com/parsiya/parsiya.net title="Source Repo" target=_blank rel="noopener noreferrer">Source Repo</a></li><li><a href="https://queue.acm.org/detail.cfm?id=3197520" title="Manual Work is a Bug" target=_blank rel="noopener noreferrer">Manual Work is a Bug</a></li><li><a href="https://www.google.com/search?q=andrew+ridgeley" title="The Other Guy from Wham!" target=_blank rel="noopener noreferrer">The Other Guy from Wham!</a></li></ul><ul class=subscription><a href=https://parsiya.net/index.xml target=_blank type=application/rss+xml title=RSS rel="noopener noreferrer"><i class="fa fa-rss-square fa-lg"></i></a></ul><form action=https://www.google.com/search method=get target=_blank rel="noopener noreferrer"><fieldset role=search><input class=search type=text name=q results=0 placeholder=Search>
<input type=hidden name=q value=site:https://parsiya.net/></fieldset></form></nav><div id=main><div id=content><div><article class=hentry role=article><header><p class=meta>Sep 3, 2025
- 8 minute read - <a class=label href=https://parsiya.net/categories/ai/>AI</a></p><h1 class=entry-title>So You Wanna Use Your Own LLMs in GitHub Copilot Chat</h1></header><div class=entry-content><nav id=TableOfContents><ul><li><a href=#problem-statement>Problem Statement</a></li><li><a href=#summary>Summary</a></li><li><a href=#details>Details</a><ul><li><a href=#enter-litellm>Enter LiteLLM</a></li><li><a href=#chats-ollama-support>Chat's Ollama Support</a></li><li><a href=#chats-azure-ai-support>Chat's Azure AI Support</a></li><li><a href=#workflow>Workflow</a></li><li><a href=#benefits>Benefits</a></li><li><a href=#drawbacks>Drawbacks</a></li><li><a href=#qa>Q&amp;A</a></li></ul></li></ul></nav><p>We want to use custom OpenAI compatible API LLMs with GitHub Copilot Chat in VS
Code without API keys. We will use LiteLLM as a proxy for authentication and use
the Azure AI model support in Chat as a hack.</p><h1 id=problem-statement>Problem Statement
<a class=header-link href=#problem-statement><svg class="fill-current o-60 hover-accent-color-light" height="22" viewBox="0 0 24 24" width="22"><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h1><p>GitHub Copilot Chat in VS Code (moving forward, called <code>Chat</code><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>) allows custom
LLM deployments, but only supports API keys and not AAD/Entra ID. API keys are
icky and <a href=https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/secure-future-initiative-protect-identities-and-secrets.pdf target=_blank rel="noreferrer noopener">not cool anymore</a>. Using this method I can:</p><ol><li>Use my own deployment in Azure AI.</li><li>Use <del>AAD</del> Entra ID (placeholder for the eventual name change in 5 years).</li><li>As a bonus, decouple my code from LLM authentication.</li></ol><h1 id=summary>Summary
<a class=header-link href=#summary><svg class="fill-current o-60 hover-accent-color-light" height="22" viewBox="0 0 24 24" width="22"><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h1><p>If you just want the solution:</p><ol><li>Create your own deployment in Azure AI.</li><li>Setup LiteLLM with a config like this:<div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#268bd2>general_settings</span>:
</span></span><span style=display:flex><span>  <span style=color:#586e75># master_key: sk-local-proxy # optional, fake API key for LiteLLM</span>
</span></span><span style=display:flex><span>  <span style=color:#268bd2>telemetry</span>: <span style=color:#cb4b16>false</span>
</span></span><span style=display:flex><span><span style=color:#268bd2>model_list</span>:
</span></span><span style=display:flex><span>  - <span style=color:#268bd2>model_name</span>: <span style=color:#2aa198>&#34;gpt-5-parsia&#34;</span> <span style=color:#586e75># custom name that GitHub Copilot chat sees</span>
</span></span><span style=display:flex><span>    <span style=color:#268bd2>litellm_params</span>:
</span></span><span style=display:flex><span>      <span style=color:#268bd2>model</span>: <span style=color:#2aa198>&#34;azure/gpt-5-parsia&#34;</span> <span style=color:#586e75># keep &#34;azure/&#34;, the rest is the name of the deployment</span>
</span></span><span style=display:flex><span>      <span style=color:#268bd2>api_base</span>: <span style=color:#2aa198>&#34;https://{base-api}.cognitiveservices.azure.com/&#34;</span> <span style=color:#586e75># replace</span>
</span></span><span style=display:flex><span>      <span style=color:#268bd2>api_version</span>: <span style=color:#2aa198>&#34;2024-12-01-preview&#34;</span> <span style=color:#586e75># replace if needed</span>
</span></span><span style=display:flex><span><span style=color:#268bd2>litellm_settings</span>:
</span></span><span style=display:flex><span>  <span style=color:#268bd2>enable_azure_ad_token_refresh</span>: <span style=color:#cb4b16>true</span> <span style=color:#586e75># use AAD tokens</span>
</span></span><span style=display:flex><span>  <span style=color:#268bd2>drop_params</span>: <span style=color:#cb4b16>true</span> <span style=color:#586e75># keep this, otherwise you will get errors.</span>
</span></span></code></pre></div></li><li>Add this section to the VS Code config:</li></ol><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#586e75>// rest of the settings
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#268bd2>&#34;github.copilot.chat.azureModels&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#268bd2>&#34;gpt-5-parsia&#34;</span>: { <span style=color:#586e75>// This is just an identifier
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;name&#34;</span>: <span style=color:#2aa198>&#34;gpt-5-parsia&#34;</span>,   <span style=color:#586e75>// The model name you see in Chat
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;url&#34;</span>: <span style=color:#2aa198>&#34;http://localhost:4000/v1/chat/completions&#34;</span>, <span style=color:#586e75>// Point to LiteLLM
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;maxInputTokens&#34;</span>: <span style=color:#2aa198>128000</span>, <span style=color:#586e75>// Set based on your model
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;maxOutputTokens&#34;</span>: <span style=color:#2aa198>16000</span>, <span style=color:#586e75>// Ditto
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;toolCalling&#34;</span>: <span style=color:#cb4b16>true</span>,      <span style=color:#586e75>// Enable this
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;vision&#34;</span>: <span style=color:#cb4b16>false</span>,
</span></span><span style=display:flex><span>      <span style=color:#268bd2>&#34;thinking&#34;</span>: <span style=color:#cb4b16>false</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><ol><li>Create this env variable for LiteLLM:</li><li><code>AZURE_CREDENTIAL</code> with value <code>DefaultAzureCredential</code></li><li>Run LiteLLM</li><li><code>litellm --config .\config.yaml --host localhost</code></li><li>Click <code>Manage Model</code> in Chat and select <code>Azure</code>.</li><li>Choose <code>gpt-5-parsia</code>.</li></ol><p>Drawback: There's a very noticeable lag in responses compared to the built-in
models. Not that bad if you're batch processing, but not a great experience for
real-time use.</p><h1 id=details>Details
<a class=header-link href=#details><svg class="fill-current o-60 hover-accent-color-light" height="22" viewBox="0 0 24 24" width="22"><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h1><p>While I like the models in Chat, using your own model in your own subscription
opens up a lot of opportunities. I was talking to a friend at work (you've been
promoted to friend for external propaganda purposes, A) and they mentioned not
using the built-in models in Chat because of dealing with sensitive stuff (OMG,
who cares, donate it all to the magic oracle in return for visions).</p><p>Chat has support for Azure models (and other providers), but only supports API
keys. We cannot use API keys. With apologies to <a href=https://en.wikipedia.org/wiki/Hafez target=_blank rel="noreferrer noopener">Hafez</a>:</p><blockquote><p>دردم از کار است و درمان نیز هم <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>
My pain and remedy are both from work</p></blockquote><p>So I am logged into the machine which is Entra joined, and my model is deployed
in Azure, so I should be able to get a token to talk to the model, but Chat
doesn't support this natively.</p><h2 id=enter-litellm>Enter LiteLLM
<a class=header-link href=#enter-litellm><svg class="fill-current o-60 hover-accent-color-light" height="22" viewBox="0 0 24 24" width="22"><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>At DEF CON, I visited AIxCC and briefly talked with people. Looking through the
code for Trail of Bits 2nd place system, <a href=https://github.com/trailofbits/buttercup target=_blank rel="noreferrer noopener">ButterCup</a>, I saw a directory
named <a href=https://github.com/trailofbits/buttercup/tree/main/litellm target=_blank rel="noreferrer noopener">litellm</a>.</p><p><a href=https://docs.litellm.ai/docs/ target=_blank rel="noreferrer noopener">LiteLLM</a> is a local LLM proxy. It does a lot more like budgeting,
but I was only interested in <a href=https://docs.litellm.ai/docs/providers/azure#azure-ad-token-refresh---defaultazurecredential target=_blank rel="noreferrer noopener">Azure AD Token Refresh</a> support. It
uses something called <code>DefaultAzureCredential</code> to obtain a token.</p><p>Think of <code>DefaultAzureCredential</code> as a magical way of getting an AAD token. On
an Entra-joined machine, it will try a few ways to passively obtain a valid token
and if not, will show you one of those familiar "choose account" dialogs and if
all else fails, opens a browser window to let you login <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><p>So we create a LiteLLM config like this:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#586e75># Basic proxy settings</span>
</span></span><span style=display:flex><span><span style=color:#268bd2>general_settings</span>:
</span></span><span style=display:flex><span>  <span style=color:#586e75># master_key: sk-local-proxy # optional, fake API key for LiteLLM</span>
</span></span><span style=display:flex><span>  <span style=color:#268bd2>telemetry</span>: <span style=color:#cb4b16>false</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#268bd2>model_list</span>:
</span></span><span style=display:flex><span>  - <span style=color:#268bd2>model_name</span>: <span style=color:#2aa198>&#34;gpt-5-parsia&#34;</span> <span style=color:#586e75># custom name that GitHub Copilot chat sees</span>
</span></span><span style=display:flex><span>    <span style=color:#268bd2>litellm_params</span>:
</span></span><span style=display:flex><span>      <span style=color:#268bd2>model</span>: <span style=color:#2aa198>&#34;azure/gpt-5-parsia&#34;</span> <span style=color:#586e75># keep &#34;azure/&#34;, the rest is the name of the deployment</span>
</span></span><span style=display:flex><span>      <span style=color:#268bd2>api_base</span>: <span style=color:#2aa198>&#34;https://{base-api}.cognitiveservices.azure.com/&#34;</span> <span style=color:#586e75># replace</span>
</span></span><span style=display:flex><span>      <span style=color:#268bd2>api_version</span>: <span style=color:#2aa198>&#34;2024-12-01-preview&#34;</span> <span style=color:#586e75># replace if needed</span>
</span></span><span style=display:flex><span>    <span style=color:#268bd2>model_info</span>: <span style=color:#586e75># optional section but helps LiteLLM understand things</span>
</span></span><span style=display:flex><span>      <span style=color:#268bd2>base_model</span>: <span style=color:#2aa198>&#34;gpt-5&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#268bd2>mode</span>: <span style=color:#2aa198>&#34;completion&#34;</span> <span style=color:#586e75># not needed but good to have</span>
</span></span><span style=display:flex><span>      <span style=color:#586e75># more options</span>
</span></span><span style=display:flex><span>      <span style=color:#586e75># input_cost_per_token</span>
</span></span><span style=display:flex><span>      <span style=color:#586e75># output_cost_per_token</span>
</span></span><span style=display:flex><span>      <span style=color:#586e75># max_tokens</span>
</span></span><span style=display:flex><span>      <span style=color:#586e75># metadata # apparently freeform!</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># Optional router/proxy tweaks</span>
</span></span><span style=display:flex><span><span style=color:#268bd2>router_settings</span>:
</span></span><span style=display:flex><span>  <span style=color:#268bd2>num_retries</span>: <span style=color:#2aa198>2</span>
</span></span><span style=display:flex><span>  <span style=color:#268bd2>timeout</span>: <span style=color:#2aa198>120</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#268bd2>litellm_settings</span>:
</span></span><span style=display:flex><span>  <span style=color:#268bd2>enable_azure_ad_token_refresh</span>: <span style=color:#cb4b16>true</span> <span style=color:#586e75># this is where the AAD token is magically acquired</span>
</span></span><span style=display:flex><span>  <span style=color:#268bd2>drop_params</span>: <span style=color:#cb4b16>true</span> <span style=color:#586e75># keep this, otherwise you will get errors.</span>
</span></span></code></pre></div><p>Most of the config is self-explanatory. You can have multiple models in LiteLLM.
Our example only has one. You only need to replace a maximum of
three items under <code>litellm_params</code> with data from your deployment.</p><ol><li><code>model</code>: This should start with <code>azure/</code> to tell LiteLLM where it's hosted.</li><li><code>api_base</code>: Your API endpoint. This is
<code>https://{base-api}.cognitiveservices.azure.com/</code> where <code>{base-api}</code> is also
the name of your Azure AI Foundry resource.</li><li><code>api_version</code>: Comes from your deployment.</li></ol><p>The config is very extensive. For example, LiteLLM can create fake API keys
with specific budgets. These are used to talk to LiteLLM. You can also have a
set API key for Chat to talk to LiteLLM (top of the config).</p><p>Now, we can run LiteLLM and it will expose an OpenAI compatible API (e.g.,
<code>/chat/completions/</code>) which is the de facto standard these days. But we have
more work to do.</p><h2 id=chats-ollama-support>Chat's Ollama Support
<a class=header-link href=#chats-ollama-support><svg class="fill-current o-60 hover-accent-color-light" height="22" viewBox="0 0 24 24" width="22"><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>Chat supports local models, but via <a href=https://ollama.com/ target=_blank rel="noreferrer noopener">Ollama</a>. By default,
it tries to talk to <code>http://localhost:11434</code>; you can also change it with
this key in the VS Code config.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#2aa198>&#34;github.copilot.chat.byok.ollamaEndpoint&#34;</span>: <span style=color:#2aa198>&#34;http://localhost:11434&#34;</span>,
</span></span></code></pre></div><p>So we can tell Chat to talk to a custom endpoint. However, Chat is expecting an
Ollama API which is different from OpenAI compatible API exposed by LiteLLM and
used by Azure.</p><p><a href=https://www.linkedin.com/pulse/you-can-now-connect-your-own-model-github-copilot-aymen-furter-qxwdf/ target=_blank rel="noreferrer noopener">You Can Now Connect Your Own Model for GitHub Copilot</a> from March 2025
suggests running LiteLLM on <code>11434</code> and claims it can emulate an Ollama API. I
couldn't get it to work, and I could not find any switches or configurations to
tell LiteLLM to emulate the Ollama API.</p><p>So we need something to translate one to the other. Originally, I used a second
Python package named <a href=https://github.com/CNSeniorious000/oai2ollama target=_blank rel="noreferrer noopener">oai2ollama</a> that does it. So the setup looked like
this:</p><pre tabindex=0><code> .-------.      .----------.      .-------.      .--------.
| VS Code +---&gt;| oai2ollama +---&gt;| LiteLLM +---&gt;| Azure AI |
 &#39;-------&#39;      &#39;----------&#39;      &#39;-------&#39;      &#39;--------&#39;
</code></pre><h2 id=chats-azure-ai-support>Chat's Azure AI Support
<a class=header-link href=#chats-azure-ai-support><svg class="fill-current o-60 hover-accent-color-light" height="22" viewBox="0 0 24 24" width="22"><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>There's experimental support for custom Azure AI models in VS Code's Chat. If
you open settings with <code>ctrl+,</code>, you can search for <code>Azure custom</code> and see it.
You have to edit it in JSON mode and add this info.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#586e75>// rest of the settings
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#268bd2>&#34;github.copilot.chat.azureModels&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#268bd2>&#34;gpt-5-parsia&#34;</span>: { <span style=color:#586e75>// This is just an identifier
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;name&#34;</span>: <span style=color:#2aa198>&#34;gpt-5-parsia&#34;</span>,     <span style=color:#586e75>// The model name you see in Chat
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;url&#34;</span>: <span style=color:#2aa198>&#34;http://localhost:4000/v1/chat/completions&#34;</span>, <span style=color:#586e75>// Point to LiteLLM
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;maxInputTokens&#34;</span>: <span style=color:#2aa198>128000</span>, <span style=color:#586e75>// Set these based on your model
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;maxOutputTokens&#34;</span>: <span style=color:#2aa198>16000</span>, <span style=color:#586e75>// Ditto
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;toolCalling&#34;</span>: <span style=color:#cb4b16>true</span>,      <span style=color:#586e75>// Enable this
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>      <span style=color:#268bd2>&#34;vision&#34;</span>: <span style=color:#cb4b16>false</span>,
</span></span><span style=display:flex><span>      <span style=color:#268bd2>&#34;thinking&#34;</span>: <span style=color:#cb4b16>false</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=workflow>Workflow
<a class=header-link href=#workflow><svg class="fill-current o-60 hover-accent-color-light" height="22" viewBox="0 0 24 24" width="22"><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><ol><li>Run LiteLLM</li></ol><pre tabindex=0><code>litellm --config config.yaml --host localhost
</code></pre><ol start=2><li>In VS Code's Chat, click the model and select <code>Manage Models</code>.</li><li>Select Azure and you should see your model.</li><li>If Chat asks for an API key, enter any random text unless you had set up one
in LiteLLM's config. This will only be sent to LiteLLM.</li></ol><p>You should disable all other Copilot models in Chat to ensure you're only using
the AI in your own subscription. The model might change when you start a new
instance of VS Code.</p><ol><li>In Chat, click on <code>Manage Models</code>.</li><li>Select <code>Copilot</code> and remove all models.</li><li>Repeat for any other model you've setup.</li></ol><h2 id=benefits>Benefits
<a class=header-link href=#benefits><svg class="fill-current o-60 hover-accent-color-light" height="22" viewBox="0 0 24 24" width="22"><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>Now you have your own private hallucinating oracle that grants wishes in a
secure manner.</p><p>For me, the main benefit is separating my models, authentication, and tokens
from code. In the code, I just need a model name and endpoint and to quote
<a href="https://youtu.be/uPxKW7RR7h0?t=169" target=_blank rel="noreferrer noopener">Billy Connolly's HBO skit</a> "buggered if I know what happens after
that<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>."</p><h2 id=drawbacks>Drawbacks
<a class=header-link href=#drawbacks><svg class="fill-current o-60 hover-accent-color-light" height="22" viewBox="0 0 24 24" width="22"><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>It's SLOW with a very high latency of 30-40 seconds (GPT-5). And I'm not talking
about just the first request that needs to acquire a token. Subsequent requests
also have this high lag and it makes GPT-5 unusable for real time use, but it
works for batch processing as long as you are not sending more than dozens of
requests per second.</p><p>IMO, part of the latency is because GPT-5 is only available in East US2 which is a
bit away from me in PNW. See <a href="https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#global-standard-model-availability" target=_blank rel="noreferrer noopener">Global Standard model availability</a> for more
info.</p><p><strong>Experiment #1</strong>: GPT-4.1 in WestUS3 has a ~3 second delay which is on par with
built-in models in Chat and quite usable.</p><p>Another issue is that LiteLLM loads tons of features that we do not use.</p><p><strong>Future work #1</strong>, I need to find a similar product. I've looked into Portkey,
but I haven't experimented with it yet. Theoretically, you could code something
that refreshes the token and redirects the requests, but I'd rather use an
existing product in case I want to use other endpoints.</p><h2 id=qa>Q&amp;A
<a class=header-link href=#qa><svg class="fill-current o-60 hover-accent-color-light" height="22" viewBox="0 0 24 24" width="22"><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><ol><li>Does it work in Visual Studio?<ol><li>I don't know. I use VS Code. I am a normie, not a corpo.</li></ol></li><li>Why doesn't GitHub Copilot Chat have this functionality?<ol><li>Good question. I think it's a good use case. Apparently, there's a new
extension API in vscode-insiders that can be used to implement this
functionality.</li></ol></li></ol><p>As usual you know where to find me, especially if you have any better solutions.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>This also opens up fun possibilities like <code>Chat, is this true?</code>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>The original verse is "دردم از یار است و درمان نیز هم" (My pain and remedy are both from the beloved). Replacing یار (beloved) with کار (work).&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>It really doesn't matter what it does behind the scenes. It's a magical token-granting wishing well.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Hafez to Billy Connoly is quite the transition. Enjoying this "diversity of thought?"&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer><p class=meta><span class="byline author vcard">Posted by <span class=fn>Parsia</span></span><time>Sep 3, 2025</time></span></p><p class=meta><a class="basic-alignment left" href=https://parsiya.net/blog/2025-kda-echoes/ title="Kusto Detective Agency: Echoes of Deception - 0-8 Solves">Kusto Detective Agency: Echoes of Deception - 0-8 Solves</a>
<a class="basic-alignment right" href=https://parsiya.net/blog/wtf-is-ai-native-sast/ title="WTF is ... - AI-Native SAST?">WTF is ... - AI-Native SAST?</a></p></footer></article></div><aside class="sidebar thirds"><section class="first odd"><h1>Who am I?</h1><p><p>I am Parsia, a security engineer at Microsoft.</p><p>I write about application security, cryptography, static analysis, and
(of course) videogames.</p><p>Click on <a href=/about/>About Me!</a> to know more. Contact me via any of these ways.</p></p></section><ul class=sidebar-nav><li class=sidebar-nav-item><a target=_blank rel="me noopener noreferrer" href=https://infosec.exchange/@parsiya title=https://infosec.exchange/@parsiya><i class="fa fa-mastodon fa-3x"></i></a>
<a target=_blank rel="noopener noreferrer" href=https://github.com/parsiya/ title=https://github.com/parsiya/><i class="fa fa-github fa-3x"></i></a>
<a target=_blank rel="noopener noreferrer" href=https://twitter.com/cryptogangsta/ title=https://twitter.com/cryptogangsta/><i class="fa fa-twitter fa-3x"></i></a>
<a target=_blank rel="noopener noreferrer" href=https://www.linkedin.com/in/parsiya title=https://www.linkedin.com/in/parsiya><i class="fa fa-linkedin fa-3x"></i></a></li></ul><section class=odd><h1>Collections</h1><li><a href=https://parsiya.net/categories/thick-client-proxying/ title="Thick Client Proxying">Thick Client Proxying</a></li><li><a href=https://parsiya.net/categories/writeup/ title=CTFs/Writeups>CTFs/Writeups</a></li><li><a href=https://parsiya.net/categories/attack-surface-analysis/ title="Attack Surface Analysis">Attack Surface Analysis</a></li><li><a href=https://parsiya.net/categories/static-analysis/ title="Static Analysis">Static Analysis</a></li><li><a href=https://parsiya.net/categories/bug-bounty/ title="Bug Bounty">Bug Bounty</a></li><li><a href=https://parsiya.net/categories/blockchain/ title="Blockchain (lol)">Blockchain (lol)</a></li><li><a href=https://parsiya.net/categories/crypto/ title=Crypto(graphy)>Crypto(graphy)</a></li><li><a href=https://parsiya.net/categories/burp-extension/ title="Burp Extension Development">Burp Extension Development</a></li><li><a href=https://parsiya.net/categories/automation/ title=Automation>Automation</a></li><li><a href=https://parsiya.net/categories/reverse-engineering/ title="Reverse Engineering">Reverse Engineering</a></li><li><a href=https://parsiya.net/categories/winappdbg/ title="WinAppDbg (use Frida instead)">WinAppDbg (use Frida instead)</a></li><li><a href=https://awsome.pw title='AWSome.pw - S3 bucket squatting - my very "legit" branded vulnerability'>AWSome.pw - S3 bucket squatting - my very "legit" branded vulnerability</a></li></section></aside></div></div><footer role=contentinfo><p>Copyright &copy; 2025 Parsia - <a href=https://parsiya.net/license/>License</a> -
<span class=credit>Powered by <a target=_blank href=https://gohugo.io rel="noopener noreferrer">Hugo</a> and <a target=_blank href=https://github.com/parsiya/hugo-octopress/ rel="noopener noreferrer">Hugo-Octopress</a> theme.</p></footer></body></html>